{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85fe992d",
   "metadata": {},
   "source": [
    "# NLP Basics: Tokenization, Normalization, Lemmatization, and More"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ac059",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we will explore fundamental preprocessing techniques in NLP:\n",
    "1. Basics of Tokenization\n",
    "2. Space-Based Tokenization (Single and Multiple Languages)\n",
    "3. Word Normalization\n",
    "4. Case Folding\n",
    "5. Lemmatization\n",
    "6. Stemming\n",
    "7. Porter Stemmer\n",
    "8. Sentence Segmentation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85cc187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jenni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jenni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jenni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Download necessary data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d319fb5a",
   "metadata": {},
   "source": [
    "## 1. Basics of Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2736ea23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'Language', 'Processing', 'is', 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing a simple sentence\n",
    "text = \"Natural Language Processing is fascinating!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28adf4d6",
   "metadata": {},
   "source": [
    "## 2. Space-Based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "829cff24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Tokenization', 'splits', 'text', 'into', 'words.']\n"
     ]
    }
   ],
   "source": [
    "# Space-based tokenization (Simple)\n",
    "text = \"Tokenization splits text into words.\"\n",
    "tokens = text.split(\" \")\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f68372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (Multilingual): ['Este', 'es', 'un', 'texto', 'en', 'español.', '这是中文文本。']\n"
     ]
    }
   ],
   "source": [
    "# Space-based tokenization in multiple languages\n",
    "text_multilang = \"Este es un texto en español. 这是中文文本。\"\n",
    "tokens_multilang = text_multilang.split(\" \")\n",
    "print(f\"Tokens (Multilingual): {tokens_multilang}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fa9086",
   "metadata": {},
   "source": [
    "## 3. Word Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8050dde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized text: normalization helps in textprocessing tasks\n"
     ]
    }
   ],
   "source": [
    "# Normalizing by removing punctuation and converting to lowercase\n",
    "text = \"Normalization helps in Text-Processing tasks!\"\n",
    "normalized_text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "print(f\"Normalized text: {normalized_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522b262",
   "metadata": {},
   "source": [
    "## 4. Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b5981f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folded Text: case folding ensures consistency.\n"
     ]
    }
   ],
   "source": [
    "# Lowercasing all text for uniformity\n",
    "text = \"Case Folding Ensures Consistency.\"\n",
    "case_folded_text = text.lower()\n",
    "print(f\"Case Folded Text: {case_folded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde7564",
   "metadata": {},
   "source": [
    "## 5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abef4096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words: ['run', 'fly', 'better']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization example\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"flies\", \"better\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "print(f\"Lemmatized words: {lemmatized_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f82921",
   "metadata": {},
   "source": [
    "## 6. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9361dbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words: ['run', 'ran', 'fli', 'fli']\n"
     ]
    }
   ],
   "source": [
    "# Basic stemming example\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "words = [\"running\", \"ran\", \"flies\", \"flying\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(f\"Stemmed words: {stemmed_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ed91b",
   "metadata": {},
   "source": [
    "## 7. Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb301469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmed Words: ['run', 'runner', 'ran', 'easili', 'fli']\n"
     ]
    }
   ],
   "source": [
    "# Porter stemmer example\n",
    "porter_stemmer = PorterStemmer()\n",
    "words = [\"running\", \"runner\", \"ran\", \"easily\", \"flies\"]\n",
    "porter_stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "print(f\"Porter Stemmed Words: {porter_stemmed_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6087b0d",
   "metadata": {},
   "source": [
    "## 8. Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "161e28fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Natural Language Processing is fascinating.', 'It has many applications.', 'Tokenization is a key step.']\n"
     ]
    }
   ],
   "source": [
    "# Segmenting text into sentences\n",
    "text = \"Natural Language Processing is fascinating. It has many applications. Tokenization is a key step.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(f\"Sentences: {sentences}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
